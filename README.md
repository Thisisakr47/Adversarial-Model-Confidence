# Adverasial-Model-Confidence
**Mentor** : [Dr. Soumya Dutta](https://soumyadutta-cse.github.io/)
## Objective
Visualizing Impact of **Uncertainty** and **Adverasarial Attack** on **Deep Classifier Models**
## Description
This project aims to address issues related to the **quality**, **confidence** and **robustness** associated with predictions made by deep classifier models based on **convolutional neural networks**. To achieve this, a visual analytics approach has been taken to allow users to understand how *uncertainty estimation techniques* and *adversarial attacks* affect the performance of these models. The project has resulted in the development of the [Model Vizualizer Website](https://model-vizualizer.netlify.app/) that can show the behavior of the classifier under different circumstances, such as uncertainty and adversarial attack. By exploring factors such as model prediction confidence and accuracy, the tool/website can visually compare the behavior of a model under adversarial attack to that of a benign model.
***
This repository contains codes/files for :- 
 - [CNN Model](https://github.com/Thisisakr47/Adversarial-Model-Confidence/blob/main/MNIST/CNN.ipynb) used for **MNIST** dataset
 - [Alexnet Model](https://github.com/Thisisakr47/Adversarial-Model-Confidence/blob/main/STL-10/AlexNet.ipynb) used for **STL-10** dataset
 - [Source Code](https://github.com/Thisisakr47/Adversarial-Model-Confidence/tree/main/Front-End) for the **Front-End** of the [Model Vizualizer Website](https://model-vizualizer.netlify.app/)
 - [UGP Presentation](https://github.com/Thisisakr47/Adversarial-Model-Confidence/blob/main/Submissions/UGP_Presentation.pdf)
 - [UGP Report](https://github.com/Thisisakr47/Adversarial-Model-Confidence/blob/main/Submissions/UGP_Report.pdf)
 ***
 ### Contributors
 - [Ayush Kumar](https://github.com/Thisisakr47)
 - [Yashwant Mahajan](https://github.com/yashwant9860)
